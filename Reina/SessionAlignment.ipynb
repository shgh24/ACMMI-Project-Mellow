{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b361f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "SESSION_DATA_DIR = \"\n",
    "DATA_FINAL_DIR = \"\"\n",
    "LOCAL_TZ = \"US/Eastern\"\n",
    "mtrack_dir = \"\"\n",
    "\n",
    "\n",
    "print(f\"Session Data: {os.path.exists(SESSION_DATA_DIR)}\")\n",
    "print(f\"Data Final:   {os.path.exists(DATA_FINAL_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(os.listdir(SESSION_DATA_DIR))[:10]:\n",
    "    item_path = os.path.join(SESSION_DATA_DIR, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        files = os.listdir(item_path)\n",
    "        session_files = [f for f in files if f.startswith(\"session_info\")]\n",
    "        print(f\"{item}/\")\n",
    "        if session_files:\n",
    "            for f in session_files:\n",
    "                print(f\"    {f}\")\n",
    "        else:\n",
    "            print(f\"    Files: {files[:5]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_path = os.path.join(\n",
    "#     SESSION_DATA_DIR, \"MIT008\", \"session_info_20251031_134457.json\"\n",
    "# )\n",
    "\n",
    "# with open(sample_path, \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "\n",
    "# print(list(data.keys()))\n",
    "# print()\n",
    "\n",
    "# if isinstance(data, dict):\n",
    "#     for key in list(data.keys())[:10]:\n",
    "#         val = data[key]\n",
    "#         if isinstance(val, list):\n",
    "#             print(f\"{key}: list with {len(val)} items\")\n",
    "#             if len(val) > 0:\n",
    "#                 print(\n",
    "#                     f\"    First item keys: {list(val[0].keys()) if isinstance(val[0], dict) else type(val[0])}\"\n",
    "#                 )\n",
    "#         elif isinstance(val, dict):\n",
    "#             print(f\"{key}: dict with keys {list(val.keys())[:5]}\")\n",
    "#         else:\n",
    "#             print(f\"{key}: {type(val).__name__} = {str(val)[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ea066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions_file = os.path.join(SESSION_DATA_DIR, \"MIT009\", \"actions_20251031_151702.jsonl\")\n",
    "\n",
    "# actions = []\n",
    "# with open(actions_file, \"r\") as f:\n",
    "#     for line in f:\n",
    "#         if line.strip():\n",
    "#             actions.append(json.loads(line))\n",
    "\n",
    "# # unique events \n",
    "# unique = []\n",
    "# for a in actions:\n",
    "#     evt = a.get(\"action_type\")\n",
    "#     if evt not in unique:\n",
    "#         unique.append(evt)\n",
    "\n",
    "# print(\"Unique action_types (in order):\\n\")\n",
    "# for e in unique:\n",
    "#     print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0240b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(os.listdir(mtrack_dir))\n",
    "print(f\"Files in mtrack ({len(files)} total):\\n\")\n",
    "for f in files[:15]:\n",
    "    print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8fda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHASE_DEFINITIONS = {\n",
    "    \"Descriptive_Stress\": {\n",
    "        \"start\": [\"TASK_STARTED\", \"DESCRIPTIVE_TASK_STARTED\"],\n",
    "        \"end\": [\"DESCRIPTIVE_COUNTDOWN_AUTO_TRANSITION\"],\n",
    "    },\n",
    "    \"Stroop_Stress\": {\n",
    "        \"start\": [\n",
    "            \"STROOP_VIDEO_STARTED_3_MIN\",\n",
    "            \"STROOP_VIDEO_STARTED\",\n",
    "            \"STROOP_TASK_STARTED\",\n",
    "        ],\n",
    "        \"end\": [\"STROOP_VIDEO_END_TRANSITION\"],\n",
    "    },\n",
    "    \"Math_Stress\": {\n",
    "        \"start\": [\"MATH_TASK_STARTED\"],\n",
    "        \"end\": [\"MATH_COUNTDOWN_AUTO_TRANSITION\"],\n",
    "    },\n",
    "    \"MollyIntervention\": {\n",
    "        \"start\": [\"CONTENT_PERFORMANCE_SCREEN_DISPLAYED\"],\n",
    "        \"end\": [\"CONTENT_PERFORMANCE_COMPLETED\"],\n",
    "    },\n",
    "    \"Post-Relaxation\": {\n",
    "        \"start\": [\"POST_STUDY_COUNTDOWN_STARTED\"],\n",
    "        \"end\": [\"POST_STUDY_COUNTDOWN_AUTO_TRANSITION\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ac619",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_DURATIONS = {\n",
    "    \"Baseline\": (5, 15),  # 7 min ish\n",
    "    \"Descriptive_Stress\": (4, 7),  # 5 min\n",
    "    \"Stroop_Stress\": (1, 5),  # 2-3 min \n",
    "    \"Math_Stress\": (4, 7),  # 5 min \n",
    "    \"MollyIntervention\": (1, 15),  \n",
    "    \"Post-Relaxation\": (5, 12),  # 7 min ish \n",
    "}\n",
    "\n",
    "MAX_CAPS = {\n",
    "    \"Post-Relaxation\": 12,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_files = glob.glob(os.path.join(mtrack_dir, \"MIT*_SCENARIO_*.xlsx\"))\n",
    "participant_ids = sorted(\n",
    "    set(re.match(r\"(MIT\\d+)\", os.path.basename(f)).group(1) for f in xlsx_files)\n",
    ")\n",
    "\n",
    "print(f\"{len(participant_ids)} participants: {participant_ids}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions = []\n",
    "results = {\"success\": [], \"failed\": [], \"truncated\": []}\n",
    "\n",
    "for participant_id in participant_ids:\n",
    "    print(f\"\\n{participant_id}:\")\n",
    "\n",
    "    # ------ Load actions jsonl -----------------\n",
    "    participant_folder = os.path.join(SESSION_DATA_DIR, participant_id)\n",
    "    actions_files = glob.glob(os.path.join(participant_folder, \"actions_*.jsonl\"))\n",
    "\n",
    "    if not actions_files:\n",
    "        results[\"failed\"].append(participant_id)\n",
    "        continue\n",
    "\n",
    "    actions = []\n",
    "    with open(actions_files[0], \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                actions.append(json.loads(line))\n",
    "\n",
    "    # -------- Load mtrack xlsx ---\n",
    "    xlsx_pattern = os.path.join(mtrack_dir, f\"{participant_id}_SCENARIO_*.xlsx\")\n",
    "    xlsx_matches = glob.glob(xlsx_pattern)\n",
    "\n",
    "    df_xlsx = None\n",
    "    if xlsx_matches:\n",
    "        df_xlsx = pd.read_excel(xlsx_matches[0])\n",
    "        df_xlsx[\"start_date\"] = pd.to_datetime(df_xlsx[\"start_date\"])\n",
    "        df_xlsx[\"end_date\"] = pd.to_datetime(df_xlsx[\"end_date\"])\n",
    "        df_xlsx[\"start_date\"] = (\n",
    "            df_xlsx[\"start_date\"].dt.tz_localize(None).dt.tz_localize(LOCAL_TZ)\n",
    "        )\n",
    "        df_xlsx[\"end_date\"] = (\n",
    "            df_xlsx[\"end_date\"].dt.tz_localize(None).dt.tz_localize(LOCAL_TZ)\n",
    "        )\n",
    "        df_xlsx[\"duration_sec\"] = (\n",
    "            df_xlsx[\"end_date\"] - df_xlsx[\"start_date\"]\n",
    "        ).dt.total_seconds()\n",
    "\n",
    "    # --- Extract phases ---\n",
    "    phases = []\n",
    "\n",
    "    # baseline\n",
    "    baseline_start = None\n",
    "    baseline_end = None\n",
    "\n",
    "    for a in actions:\n",
    "        if a.get(\"action_type\") == \"RELAXATION_COUNTDOWN_STARTED\":\n",
    "            ts = a.get(\"timestamp\", {}).get(\"local\")\n",
    "            if ts:\n",
    "                baseline_start = pd.to_datetime(ts)\n",
    "                if baseline_start.tzinfo is None:\n",
    "                    baseline_start = baseline_start.tz_localize(LOCAL_TZ)\n",
    "            break\n",
    "\n",
    "    if df_xlsx is not None:\n",
    "        rest_rows = df_xlsx[df_xlsx[\"rsrc_id\"] == \"rest\"]\n",
    "        if not rest_rows.empty:\n",
    "            longest_rest = rest_rows.loc[rest_rows[\"duration_sec\"].idxmax()]\n",
    "            baseline_end = longest_rest[\"end_date\"]\n",
    "\n",
    "    if baseline_start and baseline_end:\n",
    "        duration_s = (baseline_end - baseline_start).total_seconds()\n",
    "        phases.append(\n",
    "            {\n",
    "                \"phase_name\": \"Baseline\",\n",
    "                \"phase_start\": baseline_start,\n",
    "                \"phase_end\": baseline_end,\n",
    "                \"phase_duration_s\": duration_s,\n",
    "            }\n",
    "        )\n",
    "        print(f\"baseline: {duration_s/60:.1f} min\")\n",
    "    else:\n",
    "        print(f\"baseline: NOT FOUND!!\")\n",
    "\n",
    "    for phase_name, events in PHASE_DEFINITIONS.items():\n",
    "        start_action = None\n",
    "        end_action = None\n",
    "\n",
    "        for start_evt in events[\"start\"]:\n",
    "            for a in actions:\n",
    "                if a.get(\"action_type\") == start_evt:\n",
    "                    start_action = a\n",
    "                    break\n",
    "            if start_action:\n",
    "                break\n",
    "\n",
    "        if start_action:\n",
    "            start_idx = actions.index(start_action)\n",
    "            for end_evt in events[\"end\"]:\n",
    "                for a in actions[start_idx:]:\n",
    "                    if a.get(\"action_type\") == end_evt:\n",
    "                        end_action = a\n",
    "                        break\n",
    "                if end_action:\n",
    "                    break\n",
    "\n",
    "        if start_action and end_action:\n",
    "            start_ts = start_action.get(\"timestamp\", {}).get(\"local\")\n",
    "            end_ts = end_action.get(\"timestamp\", {}).get(\"local\")\n",
    "\n",
    "            if start_ts and end_ts:\n",
    "                start_dt = pd.to_datetime(start_ts)\n",
    "                end_dt = pd.to_datetime(end_ts)\n",
    "                if start_dt.tzinfo is None:\n",
    "                    start_dt = start_dt.tz_localize(LOCAL_TZ)\n",
    "                if end_dt.tzinfo is None:\n",
    "                    end_dt = end_dt.tz_localize(LOCAL_TZ)\n",
    "\n",
    "                duration_s = (end_dt - start_dt).total_seconds()\n",
    "                duration_min = duration_s / 60\n",
    "\n",
    "                if phase_name in MAX_CAPS and duration_min > MAX_CAPS[phase_name]:\n",
    "                    original_min = duration_min\n",
    "                    duration_s = MAX_CAPS[phase_name] * 60\n",
    "                    end_dt = start_dt + pd.Timedelta(seconds=duration_s)\n",
    "                    print(\n",
    "                        f\"  {phase_name}: {original_min:.1f} min -> TRUNCATED  {MAX_CAPS[phase_name]} min\"\n",
    "                    )\n",
    "                    results[\"truncated\"].append(\n",
    "                        (participant_id, phase_name, original_min)\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"  {phase_name}: {duration_min:.1f} min\")\n",
    "\n",
    "                phases.append(\n",
    "                    {\n",
    "                        \"phase_name\": phase_name,\n",
    "                        \"phase_start\": start_dt,\n",
    "                        \"phase_end\": end_dt,\n",
    "                        \"phase_duration_s\": duration_s,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(f\"  {phase_name}: NOT FOUND\")\n",
    "\n",
    "    if not phases:\n",
    "        results[\"failed\"].append(participant_id)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    phases = sorted(phases, key=lambda x: x[\"phase_start\"])\n",
    "\n",
    "    total_start = phases[0][\"phase_start\"]  # Baseline start\n",
    "    total_end = phases[-1][\"phase_end\"]\n",
    "    total_duration_s = (total_end - total_start).total_seconds()\n",
    "    session_id = total_start.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    rows = []\n",
    "    for p in phases:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"participant_id\": participant_id,\n",
    "                \"session_id\": session_id,\n",
    "                \"phase_name\": p[\"phase_name\"],\n",
    "                \"phase_start\": p[\"phase_start\"].strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\n",
    "                \"phase_end\": p[\"phase_end\"].strftime(\"%Y-%m-%dT%H:%M:%S.%f\"),\n",
    "                \"phase_duration_s\": round(p[\"phase_duration_s\"], 6),\n",
    "                \"total_duration_s\": round(total_duration_s, 6),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    session_df = pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    output_folder = os.path.join(DATA_FINAL_DIR, participant_id)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, \"session.csv\")\n",
    "    session_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"success:{len(results['success'])}\")\n",
    "print(f\"failed: {len(results['failed'])}\")\n",
    "if results[\"failed\"]:\n",
    "    print(f\"{results['failed']}\")\n",
    "if results[\"truncated\"]:\n",
    "    print(f\"Truncated phases:\")\n",
    "    for pid, phase, orig in results[\"truncated\"]:\n",
    "        print(f\"  {pid} {phase}: {orig:.1f} min, capped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_sessions:\n",
    "    combined_df = pd.concat(all_sessions, ignore_index=True)\n",
    "    combined_path = os.path.join(DATA_FINAL_DIR, \"session_all_participants.csv\")\n",
    "    combined_df.to_csv(combined_path, index=False)\n",
    "\n",
    "\n",
    "    print(f\"participants:{combined_df['participant_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a252be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_to_update = [\n",
    "    \"MIT001\",\n",
    "    \"MIT002\",\n",
    "    \"MIT003\",\n",
    "    \"MIT004\",\n",
    "    \"MIT006\",\n",
    "    \"MIT007\",\n",
    "    \"MIT008\",\n",
    "    \"MIT009\",\n",
    "    \"MIT010\",\n",
    "]\n",
    "\n",
    "updated = []\n",
    "skipped = []\n",
    "\n",
    "for pid in participants_to_update:\n",
    "    session_path = os.path.join(DATA_FINAL_DIR, pid, \"session.csv\")\n",
    "\n",
    "    if not os.path.exists(session_path):\n",
    "        print(f\"{pid}: session.csv not found\")\n",
    "        skipped.append(pid)\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(session_path)\n",
    "\n",
    "    # session_duration_s -> total_duration_s\n",
    "    if \"session_duration_s\" in df.columns:\n",
    "        df = df.rename(columns={\"session_duration_s\": \"total_duration_s\"})\n",
    "\n",
    "\n",
    "    cols_to_drop = [c for c in [\"session_start\", \"session_end\"] if c in df.columns]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "    df.to_csv(session_path, index=False)\n",
    "    print(f\"{pid}: updated ({len(df)} rows)\")\n",
    "    updated.append(pid)\n",
    "    \n",
    "if skipped:\n",
    "    print(f\"  {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_session_files = glob.glob(os.path.join(DATA_FINAL_DIR, \"MIT*\", \"session.csv\"))\n",
    "\n",
    "print(f\"Found {len(all_session_files)} session.csv files\\n\")\n",
    "\n",
    "all_sessions = []\n",
    "\n",
    "for session_path in sorted(all_session_files):\n",
    "    pid = os.path.basename(os.path.dirname(session_path))\n",
    "    df = pd.read_csv(session_path)\n",
    "    all_sessions.append(df)\n",
    "    print(f\"{pid}: {len(df)} phases\")\n",
    "\n",
    "\n",
    "combined_df = pd.concat(all_sessions, ignore_index=True)\n",
    "combined_path = os.path.join(DATA_FINAL_DIR, \"session_all_participants.csv\")\n",
    "combined_df.to_csv(combined_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
