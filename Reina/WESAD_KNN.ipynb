{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import neurokit2 as nk\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"\"\n",
    "\n",
    "subjects = [\n",
    "    \"S2\",\n",
    "    \"S3\",\n",
    "    \"S4\",\n",
    "  #  \"S5\",\n",
    "    \"S6\",\n",
    "    \"S7\",\n",
    "    \"S8\",\n",
    "    \"S9\",\n",
    "    \"S10\",\n",
    "    \"S11\", #?\n",
    "    \"S13\",\n",
    "    \"S14\",\n",
    "    \"S15\", \n",
    "    \"S16\",\n",
    "    \"S17\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb855bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [1, 2, 3]\n",
    "label_names = {1: \"Baseline\", 2: \"Stress\", 3: \"Amusement\"}\n",
    "\n",
    "#---------------------------------\n",
    "sf_chest = 700\n",
    "sf_wrist_BVP = 64\n",
    "sf_wrist_EDA = 4\n",
    "sf_wrist_TEMP = 4\n",
    "sf_wrist_ACC = 32\n",
    "\n",
    "\n",
    "window_size_sec = 60\n",
    "step_size_sec = 30\n",
    "\n",
    "# respiban\n",
    "chest_sensors = [\n",
    "    \"chest_acc_x\",\n",
    "    \"chest_acc_y\",\n",
    "    \"chest_acc_z\",\n",
    "    \"ecg\",\n",
    "    \"emg\",\n",
    "    \"chest_eda\",\n",
    "    \"chest_temp\",\n",
    "    \"resp\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0afbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nan_slope(x):\n",
    "    x = np.asarray(x)\n",
    "    idx = np.arange(len(x))\n",
    "    mask = np.isfinite(x)\n",
    "    if mask.sum() < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        s, _ = np.polyfit(idx[mask], x[mask], 1)\n",
    "        return float(s)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def _stats(signal, prefix):\n",
    "    s = np.asarray(signal, dtype=float)\n",
    "    out = {\n",
    "        f\"{prefix}_mean\": np.nanmean(s) if s.size else 0.0,\n",
    "        f\"{prefix}_std\": np.nanstd(s) if s.size else 0.0,\n",
    "        f\"{prefix}_min\": np.nanmin(s) if s.size else 0.0,\n",
    "        f\"{prefix}_max\": np.nanmax(s) if s.size else 0.0,\n",
    "        f\"{prefix}_median\": np.nanmedian(s) if s.size else 0.0,\n",
    "    }\n",
    "    out[f\"{prefix}_range\"] = out[f\"{prefix}_max\"] - out[f\"{prefix}_min\"]\n",
    "    if s.size:\n",
    "        q25 = np.nanpercentile(s, 25)\n",
    "        q75 = np.nanpercentile(s, 75)\n",
    "        out[f\"{prefix}_q25\"] = q25\n",
    "        out[f\"{prefix}_q75\"] = q75\n",
    "        out[f\"{prefix}_iqr\"] = q75 - q25\n",
    "        out[f\"{prefix}_slope\"] = _nan_slope(s)\n",
    "    else:\n",
    "        out.update(\n",
    "            {\n",
    "                f\"{prefix}_q25\": 0.0,\n",
    "                f\"{prefix}_q75\": 0.0,\n",
    "                f\"{prefix}_iqr\": 0.0,\n",
    "                f\"{prefix}_slope\": 0.0,\n",
    "            }\n",
    "        )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0367708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ecg_features_nk(ecg_window, fs=700):\n",
    "    out_keys = [\n",
    "        \"hr_mean\",\n",
    "        \"hr_std\",\n",
    "        \"hr_min\",\n",
    "        \"hr_max\",\n",
    "        \"hr_range\",\n",
    "        \"hrv_sdnn\",\n",
    "        \"hrv_rmssd\",\n",
    "        \"hrv_pnn50\",\n",
    "        \"hrv_meannn\",\n",
    "        \"hrv_sdsd\",\n",
    "        \"hrv_lf\",\n",
    "        \"hrv_hf\",\n",
    "        \"hrv_lf_hf_ratio\",\n",
    "        \"hrv_total_power\",\n",
    "        \"ecg_n_peaks\",\n",
    "        \"ecg_quality_mean\",\n",
    "        \"ecg_quality_std\",\n",
    "    ]\n",
    "    features = {k: 0.0 for k in out_keys}\n",
    "    s = np.asarray(ecg_window, dtype=float)\n",
    "    if s.size < int(5 * fs):\n",
    "        return features\n",
    "\n",
    "    try:\n",
    "        ecg_cleaned = nk.ecg_clean(s, sampling_rate=fs)\n",
    "        peaks_df, info = nk.ecg_peaks(ecg_cleaned, sampling_rate=fs)\n",
    "        hr = nk.ecg_rate(peaks_df, sampling_rate=fs, desired_length=len(ecg_cleaned))\n",
    "\n",
    "        features[\"hr_mean\"] = float(np.nanmean(hr))\n",
    "        features[\"hr_std\"] = float(np.nanstd(hr))\n",
    "        features[\"hr_min\"] = float(np.nanmin(hr))\n",
    "        features[\"hr_max\"] = float(np.nanmax(hr))\n",
    "        features[\"hr_range\"] = features[\"hr_max\"] - features[\"hr_min\"]\n",
    "\n",
    "        try:\n",
    "            hrv_time = nk.hrv_time(peaks_df, sampling_rate=fs)\n",
    "            if not hrv_time.empty:\n",
    "                features[\"hrv_sdnn\"] = float(hrv_time.get(\"HRV_SDNN\", [0]).values[0])\n",
    "                features[\"hrv_rmssd\"] = float(hrv_time.get(\"HRV_RMSSD\", [0]).values[0])\n",
    "                features[\"hrv_pnn50\"] = float(hrv_time.get(\"HRV_pNN50\", [0]).values[0])\n",
    "                features[\"hrv_meannn\"] = float(\n",
    "                    hrv_time.get(\"HRV_MeanNN\", [0]).values[0]\n",
    "                )\n",
    "                features[\"hrv_sdsd\"] = float(hrv_time.get(\"HRV_SDSD\", [0]).values[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            hrv_freq = nk.hrv_frequency(\n",
    "                peaks_df, sampling_rate=fs, method=\"fft\", show=False\n",
    "            )\n",
    "            if not hrv_freq.empty:\n",
    "                features[\"hrv_lf\"] = float(hrv_freq.get(\"HRV_LF\", [0]).values[0])\n",
    "                features[\"hrv_hf\"] = float(hrv_freq.get(\"HRV_HF\", [0]).values[0])\n",
    "                features[\"hrv_lf_hf_ratio\"] = float(\n",
    "                    hrv_freq.get(\"HRV_LFHF\", [0]).values[0]\n",
    "                )\n",
    "                features[\"hrv_total_power\"] = float(\n",
    "                    hrv_freq.get(\"HRV_TP\", [0]).values[0]\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        rpeaks_idx = info.get(\"ECG_R_Peaks\", [])\n",
    "        features[\"ecg_n_peaks\"] = float(len(rpeaks_idx))\n",
    "        features[\"ecg_quality_mean\"] = float(np.nanmean(ecg_cleaned))\n",
    "        features[\"ecg_quality_std\"] = float(np.nanstd(ecg_cleaned))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return features\n",
    "\n",
    "#---------------------------------\n",
    "def extract_eda_features_nk(eda_window, fs, prefix=\"\"):\n",
    " \"\"\"eda\"\"\"\n",
    "    s = np.asarray(eda_window, dtype=float)\n",
    "    out = {}\n",
    "    try:\n",
    "        eda_clean = nk.eda_clean(s, sampling_rate=fs)\n",
    "        phasic = nk.eda_phasic(eda_clean, sampling_rate=fs)\n",
    "        tonic = phasic[\"EDA_Tonic\"].values\n",
    "        ph = phasic[\"EDA_Phasic\"].values\n",
    "\n",
    "        out.update(_stats(tonic, f\"{prefix}eda_tonic\"))\n",
    "        out.update(_stats(ph, f\"{prefix}eda_phasic\"))\n",
    "\n",
    "        peaks_dict, peaks_info = nk.eda_peaks(ph, sampling_rate=fs)\n",
    "        out[f\"{prefix}eda_scr_n_peaks\"] = float(\n",
    "            np.nansum(peaks_dict.get(\"SCR_Peaks\", 0))\n",
    "        )\n",
    "\n",
    "        amps = peaks_info.get(\"SCR_Amplitude\", [])\n",
    "        amps = (\n",
    "            np.asarray([a for a in amps if np.isfinite(a)], dtype=float)\n",
    "            if len(amps)\n",
    "            else np.array([])\n",
    "        )\n",
    "        out[f\"{prefix}eda_scr_amplitude_mean\"] = (\n",
    "            float(np.nanmean(amps)) if amps.size else 0.0\n",
    "        )\n",
    "        out[f\"{prefix}eda_scr_amplitude_max\"] = (\n",
    "            float(np.nanmax(amps)) if amps.size else 0.0\n",
    "        )\n",
    "        out[f\"{prefix}eda_scr_amplitude_sum\"] = (\n",
    "            float(np.nansum(amps)) if amps.size else 0.0\n",
    "        )\n",
    "\n",
    "        out.update(_stats(eda_clean, f\"{prefix}eda\"))\n",
    "    except:\n",
    "        out.update(_stats(s, f\"{prefix}eda\"))\n",
    "        zeros = [\n",
    "            \"tonic_mean\",\n",
    "            \"tonic_std\",\n",
    "            \"tonic_min\",\n",
    "            \"tonic_max\",\n",
    "            \"tonic_range\",\n",
    "            \"tonic_median\",\n",
    "            \"tonic_q25\",\n",
    "            \"tonic_q75\",\n",
    "            \"tonic_iqr\",\n",
    "            \"tonic_slope\",\n",
    "            \"phasic_mean\",\n",
    "            \"phasic_std\",\n",
    "            \"phasic_min\",\n",
    "            \"phasic_max\",\n",
    "            \"phasic_range\",\n",
    "            \"phasic_median\",\n",
    "            \"phasic_q25\",\n",
    "            \"phasic_q75\",\n",
    "            \"phasic_iqr\",\n",
    "            \"phasic_slope\",\n",
    "        ]\n",
    "        for k in zeros:\n",
    "            out[f\"{prefix}eda_{k}\"] = 0.0\n",
    "        out[f\"{prefix}eda_scr_n_peaks\"] = 0.0\n",
    "        out[f\"{prefix}eda_scr_amplitude_mean\"] = 0.0\n",
    "        out[f\"{prefix}eda_scr_amplitude_max\"] = 0.0\n",
    "        out[f\"{prefix}eda_scr_amplitude_sum\"] = 0.0\n",
    "    return out\n",
    "\n",
    "#---------------------------------#---------------------------------\n",
    "def extract_rsp_features_nk(rsp_window, fs=700):\n",
    "    \"\"\"respiration\"\"\"\n",
    "    s = np.asarray(rsp_window, dtype=float)\n",
    "    out = {\n",
    "        k: 0.0\n",
    "        for k in [\n",
    "            \"rsp_rate_mean\",\n",
    "            \"rsp_rate_std\",\n",
    "            \"rsp_rate_min\",\n",
    "            \"rsp_rate_max\",\n",
    "            \"rsp_rate_range\",\n",
    "            \"rsp_amplitude_mean\",\n",
    "            \"rsp_amplitude_std\",\n",
    "            \"rsp_amplitude_max\",\n",
    "            \"rrv_sdbb\",\n",
    "            \"rsp_mean\",\n",
    "            \"rsp_std\",\n",
    "            \"rsp_min\",\n",
    "            \"rsp_max\",\n",
    "            \"rsp_range\",\n",
    "            \"rsp_median\",\n",
    "            \"rsp_q25\",\n",
    "            \"rsp_q75\",\n",
    "            \"rsp_iqr\",\n",
    "            \"rsp_slope\",\n",
    "        ]\n",
    "    }\n",
    "    if s.size < int(3 * fs):\n",
    "        return out\n",
    "    try:\n",
    "        rsp_clean = nk.rsp_clean(s, sampling_rate=fs)\n",
    "        sig, info = nk.rsp_process(rsp_clean, sampling_rate=fs)\n",
    "        rate = sig.get(\"RSP_Rate\", np.array([]))\n",
    "        amp = sig.get(\"RSP_Amplitude\", np.array([]))\n",
    "\n",
    "        if len(rate):\n",
    "            out[\"rsp_rate_mean\"] = float(np.nanmean(rate))\n",
    "            out[\"rsp_rate_std\"] = float(np.nanstd(rate))\n",
    "            out[\"rsp_rate_min\"] = float(np.nanmin(rate))\n",
    "            out[\"rsp_rate_max\"] = float(np.nanmax(rate))\n",
    "            out[\"rsp_rate_range\"] = out[\"rsp_rate_max\"] - out[\"rsp_rate_min\"]\n",
    "\n",
    "        if len(amp):\n",
    "            out[\"rsp_amplitude_mean\"] = float(np.nanmean(amp))\n",
    "            out[\"rsp_amplitude_std\"] = float(np.nanstd(amp))\n",
    "            out[\"rsp_amplitude_max\"] = float(np.nanmax(amp))\n",
    "\n",
    "        try:\n",
    "            rrv = nk.rsp_rrv(sig, sampling_rate=fs)\n",
    "            if not rrv.empty:\n",
    "                out[\"rrv_sdbb\"] = float(rrv.get(\"RRV_SDBB\", [0]).values[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        out.update(_stats(rsp_clean, \"rsp\"))\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "#---------------------------------#---------------------------------#---------------------------------#---------------------------------\n",
    "def extract_emg_features_nk(emg_window, fs=700):\n",
    "    \"\"\"EMG\"\"\"\n",
    "    s = np.asarray(emg_window, dtype=float)\n",
    "    out = {\n",
    "        k: 0.0\n",
    "        for k in [\n",
    "            \"emg_amplitude_mean\",\n",
    "            \"emg_amplitude_std\",\n",
    "            \"emg_amplitude_max\",\n",
    "            \"emg_amplitude_min\",\n",
    "            \"emg_amplitude_range\",\n",
    "            \"emg_n_onsets\",\n",
    "            \"emg_rms\",\n",
    "            \"emg_mean\",\n",
    "            \"emg_std\",\n",
    "            \"emg_min\",\n",
    "            \"emg_max\",\n",
    "            \"emg_range\",\n",
    "            \"emg_median\",\n",
    "            \"emg_q25\",\n",
    "            \"emg_q75\",\n",
    "            \"emg_iqr\",\n",
    "            \"emg_slope\",\n",
    "        ]\n",
    "    }\n",
    "    if s.size < int(1 * fs):\n",
    "        return out\n",
    "    try:\n",
    "        emg_clean = nk.emg_clean(s, sampling_rate=fs)\n",
    "        sig, info = nk.emg_process(emg_clean, sampling_rate=fs)\n",
    "        amp_series = sig.get(\"EMG_Amplitude\", sig.get(\"EMG_Envelope\", np.array([])))\n",
    "        amp = np.asarray(amp_series) if amp_series is not None else np.array([])\n",
    "\n",
    "        if amp.size:\n",
    "            out[\"emg_amplitude_mean\"] = float(np.nanmean(amp))\n",
    "            out[\"emg_amplitude_std\"] = float(np.nanstd(amp))\n",
    "            out[\"emg_amplitude_max\"] = float(np.nanmax(amp))\n",
    "            out[\"emg_amplitude_min\"] = float(np.nanmin(amp))\n",
    "            out[\"emg_amplitude_range\"] = (\n",
    "                out[\"emg_amplitude_max\"] - out[\"emg_amplitude_min\"]\n",
    "            )\n",
    "\n",
    "        onsets = sig.get(\"EMG_Onsets\", np.array([]))\n",
    "        out[\"emg_n_onsets\"] = float(np.nansum(onsets)) if len(onsets) else 0.0\n",
    "        out[\"emg_rms\"] = (\n",
    "            float(np.sqrt(np.nanmean(emg_clean**2))) if emg_clean.size else 0.0\n",
    "        )\n",
    "        out.update(_stats(emg_clean, \"emg\"))\n",
    "    except:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "#---------------------------------#---------------------------------\n",
    "def extract_bvp_features_nk(bvp_window, fs=64):\n",
    "    \"\"\"BVP\"\"\" \n",
    "    s = np.asarray(bvp_window, dtype=float)\n",
    "    out = {\n",
    "        k: 0.0\n",
    "        for k in [\n",
    "            \"bvp_rate_mean\",\n",
    "            \"bvp_rate_std\",\n",
    "            \"bvp_rate_min\",\n",
    "            \"bvp_rate_max\",\n",
    "            \"bvp_rate_range\",\n",
    "            \"bvp_n_peaks\",\n",
    "            \"bvp_mean\",\n",
    "            \"bvp_std\",\n",
    "            \"bvp_min\",\n",
    "            \"bvp_max\",\n",
    "            \"bvp_range\",\n",
    "            \"bvp_median\",\n",
    "            \"bvp_q25\",\n",
    "            \"bvp_q75\",\n",
    "            \"bvp_iqr\",\n",
    "            \"bvp_slope\",\n",
    "        ]\n",
    "    }\n",
    "    if s.size < 10:\n",
    "        return out\n",
    "    try:\n",
    "        ppg_clean = nk.ppg_clean(s, sampling_rate=fs)\n",
    "        sig, info = nk.ppg_process(ppg_clean, sampling_rate=fs)\n",
    "        rate = sig[\"PPG_Rate\"].values\n",
    "        peaks = sig[\"PPG_Peaks\"].values\n",
    "\n",
    "        out[\"bvp_rate_mean\"] = float(np.nanmean(rate))\n",
    "        out[\"bvp_rate_std\"] = float(np.nanstd(rate))\n",
    "        out[\"bvp_rate_min\"] = float(np.nanmin(rate))\n",
    "        out[\"bvp_rate_max\"] = float(np.nanmax(rate))\n",
    "        out[\"bvp_rate_range\"] = out[\"bvp_rate_max\"] - out[\"bvp_rate_min\"]\n",
    "        out[\"bvp_n_peaks\"] = float(np.nansum(peaks))\n",
    "\n",
    "        out.update(_stats(ppg_clean, \"bvp\"))\n",
    "    except:\n",
    "        pass\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c410820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing val \n",
    "chest_missing = pd.DataFrame(\n",
    "    {\n",
    "        \"Column\": chest_sensors,\n",
    "        \"Missing_Count\": [\n",
    "            df_chest_combined[col].isnull().sum() for col in chest_sensors\n",
    "        ],\n",
    "        \"Missing_Percent\": [\n",
    "            df_chest_combined[col].isnull().sum() / len(df_chest_combined) * 100\n",
    "            for col in chest_sensors\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(chest_missing.to_string(index=False))\n",
    "\n",
    "wrist_missing = pd.DataFrame(\n",
    "    {\n",
    "        \"Sensor\": [\"BVP\", \"EDA\", \"TEMP\", \"ACC_X\", \"ACC_Y\", \"ACC_Z\"],\n",
    "        \"Sampling_Rate\": [\n",
    "            sf_wrist_BVP,\n",
    "            sf_wrist_EDA,\n",
    "            sf_wrist_TEMP,\n",
    "            sf_wrist_ACC,\n",
    "            sf_wrist_ACC,\n",
    "            sf_wrist_ACC,\n",
    "        ],\n",
    "        \"Total_Samples\": [\n",
    "            len(df_wrist_bvp_combined),\n",
    "            len(df_wrist_eda_combined),\n",
    "            len(df_wrist_temp_combined),\n",
    "            len(df_wrist_acc_combined),\n",
    "            len(df_wrist_acc_combined),\n",
    "            len(df_wrist_acc_combined),\n",
    "        ],\n",
    "        \"Missing_Count\": [\n",
    "            df_wrist_bvp_combined[\"bvp\"].isnull().sum(),\n",
    "            df_wrist_eda_combined[\"wrist_eda\"].isnull().sum(),\n",
    "            df_wrist_temp_combined[\"wrist_temp\"].isnull().sum(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_x\"].isnull().sum(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_y\"].isnull().sum(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_z\"].isnull().sum(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "wrist_missing[\"missing&\"] = (\n",
    "    wrist_missing[\"count\"] / wrist_missing[\"Total_Samples\"] * 100\n",
    ")\n",
    "print(wrist_missing.to_string(index=False))\n",
    "\n",
    "if (\n",
    "    chest_missing[\"count\"].sum() == 0\n",
    "    and wrist_missing[\"count\"].sum() == 0\n",
    "):\n",
    "    print(\"fine\")\n",
    "    \n",
    "    \n",
    "#INF check\n",
    "chest_inf = pd.DataFrame(\n",
    "    {\n",
    "        \"Column\": chest_sensors,\n",
    "        \"Inf_Count\": [np.isinf(df_chest_combined[col]).sum() for col in chest_sensors],\n",
    "        \"NegInf_Count\": [\n",
    "            np.isneginf(df_chest_combined[col]).sum() for col in chest_sensors\n",
    "        ],\n",
    "        \"PosInf_Count\": [\n",
    "            np.isposinf(df_chest_combined[col]).sum() for col in chest_sensors\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(chest_inf.to_string(index=False))\n",
    "\n",
    "wrist_inf = pd.DataFrame(\n",
    "    {\n",
    "        \"Sensor\": [\"BVP\", \"EDA\", \"TEMP\", \"ACC_X\", \"ACC_Y\", \"ACC_Z\"],\n",
    "        \"Inf_Count\": [\n",
    "            np.isinf(df_wrist_bvp_combined[\"bvp\"]).sum(),\n",
    "            np.isinf(df_wrist_eda_combined[\"wrist_eda\"]).sum(),\n",
    "            np.isinf(df_wrist_temp_combined[\"wrist_temp\"]).sum(),\n",
    "            np.isinf(df_wrist_acc_combined[\"wrist_acc_x\"]).sum(),\n",
    "            np.isinf(df_wrist_acc_combined[\"wrist_acc_y\"]).sum(),\n",
    "            np.isinf(df_wrist_acc_combined[\"wrist_acc_z\"]).sum(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(wrist_inf.to_string(index=False))\n",
    "\n",
    "if chest_inf[\"Inf_Count\"].sum() == 0 and wrist_inf[\"Inf_Count\"].sum() == 0:\n",
    "    print(\"fine.\")\n",
    "\n",
    "#range anomalies \n",
    "chest_ranges = pd.DataFrame(\n",
    "    {\n",
    "        \"Column\": chest_sensors,\n",
    "        \"Min\": [df_chest_combined[col].min() for col in chest_sensors],\n",
    "        \"Max\": [df_chest_combined[col].max() for col in chest_sensors],\n",
    "        \"Mean\": [df_chest_combined[col].mean() for col in chest_sensors],\n",
    "        \"Std\": [df_chest_combined[col].std() for col in chest_sensors],\n",
    "        \"Median\": [df_chest_combined[col].median() for col in chest_sensors],\n",
    "    }\n",
    ")\n",
    "print(chest_ranges.to_string(index=False))\n",
    "\n",
    "# - temp\n",
    "negative_chest_temp = (df_chest_combined[\"chest_temp\"] < 0).sum()\n",
    "wrist_ranges = pd.DataFrame(\n",
    "    {\n",
    "        \"Sensor\": [\"BVP\", \"EDA\", \"TEMP\", \"ACC_X\", \"ACC_Y\", \"ACC_Z\"],\n",
    "        \"Sampling_Rate\": [\n",
    "            sf_wrist_BVP,\n",
    "            sf_wrist_EDA,\n",
    "            sf_wrist_TEMP,\n",
    "            sf_wrist_ACC,\n",
    "            sf_wrist_ACC,\n",
    "            sf_wrist_ACC,\n",
    "        ],\n",
    "        \"Min\": [\n",
    "            df_wrist_bvp_combined[\"bvp\"].min(),\n",
    "            df_wrist_eda_combined[\"wrist_eda\"].min(),\n",
    "            df_wrist_temp_combined[\"wrist_temp\"].min(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_x\"].min(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_y\"].min(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_z\"].min(),\n",
    "        ],\n",
    "        \"Max\": [\n",
    "            df_wrist_bvp_combined[\"bvp\"].max(),\n",
    "            df_wrist_eda_combined[\"wrist_eda\"].max(),\n",
    "            df_wrist_temp_combined[\"wrist_temp\"].max(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_x\"].max(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_y\"].max(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_z\"].max(),\n",
    "        ],\n",
    "        \"Mean\": [\n",
    "            df_wrist_bvp_combined[\"bvp\"].mean(),\n",
    "            df_wrist_eda_combined[\"wrist_eda\"].mean(),\n",
    "            df_wrist_temp_combined[\"wrist_temp\"].mean(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_x\"].mean(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_y\"].mean(),\n",
    "            df_wrist_acc_combined[\"wrist_acc_z\"].mean(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(wrist_ranges.to_string(index=False))\n",
    "\n",
    "\n",
    "negative_wrist_temp = (df_wrist_temp_combined[\"wrist_temp\"] < 0).sum()\n",
    "negative_wrist_eda = (df_wrist_eda_combined[\"wrist_eda\"] < 0).sum()\n",
    "extreme_bvp_low = (df_wrist_bvp_combined[\"bvp\"] < -1000).sum()\n",
    "extreme_bvp_high = (df_wrist_bvp_combined[\"bvp\"] > 1000).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211dd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_counts = df_filtered_preview[\"label\"].value_counts().sort_index()\n",
    "overall_pct = overall_counts / overall_counts.sum() * 100\n",
    "imbalance_ratio = overall_counts.max() / max(1, overall_counts.min())\n",
    "\n",
    "for lbl in valid_labels:\n",
    "    c = int(overall_counts.get(lbl, 0))\n",
    "    p = overall_pct.get(lbl, 0.0)\n",
    "    print(f\"  {label_names[lbl]:12s}: {c:10,} ({p:5.1f}%)\")\n",
    "\n",
    "\n",
    "per_subject = (\n",
    "    df_filtered_preview.groupby([\"sid\", \"label\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(columns=valid_labels, fill_value=0)\n",
    ")\n",
    "print(per_subject)\n",
    "\n",
    "total_n = overall_counts.sum()\n",
    "class_weight = {}\n",
    "for lbl in valid_labels:\n",
    "    n_lbl = max(1, int(overall_counts.get(lbl, 0)))\n",
    "    class_weight[lbl] = total_n / (len(valid_labels) * n_lbl)\n",
    "\n",
    "for lbl, weight in class_weight.items():\n",
    "    print(f\"  {label_names[lbl]:12s}: {weight:.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = [\"#AED6F1\", \"#F1948A\", \"#F8B88B\"]\n",
    "labels_text = [label_names[i] for i in overall_counts.index]\n",
    "\n",
    "ax1.pie(\n",
    "    overall_counts.values,\n",
    "    labels=labels_text,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    colors=colors,\n",
    "    shadow=True,\n",
    "    startangle=90,\n",
    ")\n",
    "ax1.set_title(\"Raw Data Label Distribution\", fontweight=\"bold\", fontsize=12)\n",
    "\n",
    "per_subject.plot(kind=\"bar\", ax=ax2, color=colors, width=0.7)\n",
    "ax2.set_title(\"Labels per Subject (Raw Data)\", fontweight=\"bold\", fontsize=12)\n",
    "ax2.set_xlabel(\"Subject\", fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Sample Count\", fontweight=\"bold\")\n",
    "ax2.legend(labels_text, title=\"Condition\")\n",
    "ax2.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"EDA_1_label_distribution_raw.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "wrist_sensors_data = {\n",
    "    \"BVP\": df_wrist_bvp_combined[\"bvp\"],\n",
    "    \"EDA\": df_wrist_eda_combined[\"wrist_eda\"],\n",
    "    \"TEMP\": df_wrist_temp_combined[\"wrist_temp\"],\n",
    "    \"ACC_X\": df_wrist_acc_combined[\"wrist_acc_x\"],\n",
    "    \"ACC_Y\": df_wrist_acc_combined[\"wrist_acc_y\"],\n",
    "    \"ACC_Z\": df_wrist_acc_combined[\"wrist_acc_z\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83038703",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_eda_means = (\n",
    "    df_wrist_eda_combined.groupby(\"subject\")[\"wrist_eda\"].mean().sort_values()\n",
    ")\n",
    "subject_eda_stds = df_wrist_eda_combined.groupby(\"subject\")[\"wrist_eda\"].std()\n",
    "\n",
    "x = np.arange(len(subject_eda_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_outlier_summary = []\n",
    "for col in chest_sensors:\n",
    "    Q1 = df_filtered_preview[col].quantile(0.25)\n",
    "    Q3 = df_filtered_preview[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = (\n",
    "        (df_filtered_preview[col] < lower_bound)\n",
    "        | (df_filtered_preview[col] > upper_bound)\n",
    "    ).sum()\n",
    "    outlier_pct = (outliers / len(df_filtered_preview)) * 100\n",
    "\n",
    "    chest_outlier_summary.append(\n",
    "        {\n",
    "            \"Column\": col,\n",
    "            \"Outliers\": outliers,\n",
    "            \"Percent\": outlier_pct,\n",
    "            \"Lower\": lower_bound,\n",
    "            \"Upper\": upper_bound,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_chest_outliers = pd.DataFrame(chest_outlier_summary)\n",
    "print(df_chest_outliers[[\"Column\", \"Outliers\", \"Percent\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "#---------------------------------#---------------------------------\n",
    "wrist_outlier_summary = []\n",
    "for sensor_name, sensor_data in wrist_sensors_data.items():\n",
    "    Q1 = sensor_data.quantile(0.25)\n",
    "    Q3 = sensor_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = ((sensor_data < lower_bound) | (sensor_data > upper_bound)).sum()\n",
    "    outlier_pct = (outliers / len(sensor_data)) * 100\n",
    "\n",
    "    wrist_outlier_summary.append(\n",
    "        {\n",
    "            \"Sensor\": sensor_name,\n",
    "            \"Outliers\": outliers,\n",
    "            \"Percent\": outlier_pct,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_wrist_outliers = pd.DataFrame(wrist_outlier_summary)\n",
    "print(df_wrist_outliers.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b49fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_chest_combined[df_chest_combined[\"label\"].isin(valid_labels)].copy()\n",
    "\n",
    "samples_before = len(df_filtered)\n",
    "if negative_chest_temp > 0:\n",
    "    df_filtered = df_filtered[df_filtered[\"chest_temp\"] > 0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6749300",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_count = 0\n",
    "\n",
    "for subject_id in all_data.keys():\n",
    "    df_temp = all_data[subject_id][\"temp\"]\n",
    "    before = len(df_temp)\n",
    "    df_temp = df_temp[df_temp[\"wrist_temp\"] > 0].copy()\n",
    "    cleaning_count += before - len(df_temp)\n",
    "    all_data[subject_id][\"temp\"] = df_temp\n",
    "\n",
    "for subject_id in all_data.keys():\n",
    "    df_eda = all_data[subject_id][\"eda\"]\n",
    "    before = len(df_eda)\n",
    "    df_eda = df_eda[df_eda[\"wrist_eda\"] >= 0].copy()\n",
    "    cleaning_count += before - len(df_eda)\n",
    "    all_data[subject_id][\"eda\"] = df_eda\n",
    "\n",
    "\n",
    "\n",
    "for subject_id in all_data.keys():\n",
    "    df_bvp = all_data[subject_id][\"bvp\"]\n",
    "    before = len(df_bvp)\n",
    "    df_bvp = df_bvp[(df_bvp[\"bvp\"] >= -1000) & (df_bvp[\"bvp\"] <= 1000)].copy()\n",
    "    cleaning_count += before - len(df_bvp)\n",
    "    all_data[subject_id][\"bvp\"] = df_bvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject_id in all_data.keys():\n",
    "    cleaned_chest = df_filtered[df_filtered[\"sid\"] == subject_id].copy()\n",
    "    all_data[subject_id][\"chest\"] = cleaned_chest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_issues = 0\n",
    "for subject_id in all_data.keys():\n",
    "    # Check chest\n",
    "    chest_neg_temp = (all_data[subject_id][\"chest\"][\"chest_temp\"] < 0).sum()\n",
    "\n",
    "    # Check wrist\n",
    "    bvp_extreme = (\n",
    "        (all_data[subject_id][\"bvp\"][\"bvp\"] < -1000)\n",
    "        | (all_data[subject_id][\"bvp\"][\"bvp\"] > 1000)\n",
    "    ).sum()\n",
    "    eda_negative = (all_data[subject_id][\"eda\"][\"wrist_eda\"] < 0).sum()\n",
    "    temp_negative = (all_data[subject_id][\"temp\"][\"wrist_temp\"] < 0).sum()\n",
    "\n",
    "    issues = chest_neg_temp + bvp_extreme + eda_negative + temp_negative\n",
    "    total_issues += issues\n",
    "\n",
    "    if issues > 0:\n",
    "        print(\"issues found\")\n",
    "\n",
    "if total_issues == 0:\n",
    "    print(\"complete\") \n",
    "else:\n",
    "    print(f\"missing count: {total_issues}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_window_features_time_based(subject_data, start_time, end_time):\n",
    "    features = {}\n",
    "    \n",
    "    #rspi ban ---------------------------------\n",
    "    df_chest = subject_data[\"chest\"]\n",
    "    chest_mask = (df_chest[\"time\"] >= start_time) & (df_chest[\"time\"] < end_time)\n",
    "    chest_window = df_chest.loc[chest_mask]\n",
    "\n",
    "    if len(chest_window) < 100:\n",
    "        return None\n",
    "\n",
    "    labels = chest_window[\"label\"].to_numpy()\n",
    "    if labels.size == 0:\n",
    "        return None\n",
    "    vals, cnts = np.unique(labels, return_counts=True)\n",
    "    maj = vals[np.argmax(cnts)]\n",
    "    if (cnts.max() / labels.size) < 0.8:\n",
    "        return None\n",
    "    features[\"label\"] = int(maj)\n",
    "\n",
    "    # Extract chest features\n",
    "    features.update(extract_ecg_features_nk(chest_window[\"ecg\"].to_numpy(), fs=700))\n",
    "    features.update(\n",
    "        extract_eda_features_nk(\n",
    "            chest_window[\"chest_eda\"].to_numpy(), fs=700, prefix=\"chest_\"\n",
    "        )\n",
    "    )\n",
    "    features.update(extract_rsp_features_nk(chest_window[\"resp\"].to_numpy(), fs=700))\n",
    "    features.update(extract_emg_features_nk(chest_window[\"emg\"].to_numpy(), fs=700))\n",
    "    features.update(_stats(chest_window[\"chest_temp\"].to_numpy(), \"chest_temp\"))\n",
    "\n",
    "    for axis in [\"chest_acc_x\", \"chest_acc_y\", \"chest_acc_z\"]:\n",
    "        features.update(_stats(chest_window[axis].to_numpy(), axis))\n",
    "\n",
    "    chest_acc_mag = np.sqrt(\n",
    "        chest_window[\"chest_acc_x\"].to_numpy() ** 2\n",
    "        + chest_window[\"chest_acc_y\"].to_numpy() ** 2\n",
    "        + chest_window[\"chest_acc_z\"].to_numpy() ** 2\n",
    "    )\n",
    "    features.update(_stats(chest_acc_mag, \"chest_acc_mag\"))\n",
    "\n",
    "    # empatica --------------------------\n",
    "    df_bvp = subject_data[\"bvp\"]\n",
    "    m = (df_bvp[\"time\"] >= start_time) & (df_bvp[\"time\"] < end_time)\n",
    "    bw = df_bvp.loc[m, \"bvp\"].to_numpy()\n",
    "    features.update(extract_bvp_features_nk(bw, fs=64))\n",
    "\n",
    "    df_eda = subject_data[\"eda\"]\n",
    "    m = (df_eda[\"time\"] >= start_time) & (df_eda[\"time\"] < end_time)\n",
    "    ew = df_eda.loc[m, \"wrist_eda\"].to_numpy()\n",
    "    features.update(extract_eda_features_nk(ew, fs=4, prefix=\"wrist_\"))\n",
    "\n",
    "    df_temp = subject_data[\"temp\"]\n",
    "    m = (df_temp[\"time\"] >= start_time) & (df_temp[\"time\"] < end_time)\n",
    "    tw = df_temp.loc[m, \"wrist_temp\"].to_numpy()\n",
    "    features.update(_stats(tw, \"wrist_temp\"))\n",
    "\n",
    "    df_acc = subject_data[\"acc\"]\n",
    "    m = (df_acc[\"time\"] >= start_time) & (df_acc[\"time\"] < end_time)\n",
    "    if m.sum() >= 10:\n",
    "        for axis in [\"wrist_acc_x\", \"wrist_acc_y\", \"wrist_acc_z\"]:\n",
    "            features.update(_stats(df_acc.loc[m, axis].to_numpy(), axis))\n",
    "        wmag = np.sqrt(\n",
    "            df_acc.loc[m, \"wrist_acc_x\"].to_numpy() ** 2\n",
    "            + df_acc.loc[m, \"wrist_acc_y\"].to_numpy() ** 2\n",
    "            + df_acc.loc[m, \"wrist_acc_z\"].to_numpy() ** 2\n",
    "        )\n",
    "        features.update(_stats(wmag, \"wrist_acc_mag\"))\n",
    "    else:\n",
    "        for axis in [\"wrist_acc_x\", \"wrist_acc_y\", \"wrist_acc_z\", \"wrist_acc_mag\"]:\n",
    "            features.update(\n",
    "                {\n",
    "                    f\"{axis}_{k}\": 0.0\n",
    "                    for k in [\n",
    "                        \"mean\",\n",
    "                        \"std\",\n",
    "                        \"min\",\n",
    "                        \"max\",\n",
    "                        \"range\",\n",
    "                        \"median\",\n",
    "                        \"q25\",\n",
    "                        \"q75\",\n",
    "                        \"iqr\",\n",
    "                        \"slope\",\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = []\n",
    "\n",
    "for subject_id in tqdm(sorted(all_data.keys()), desc=\"extracting\"):\n",
    "    subject_data = all_data[subject_id]\n",
    "    df_chest = subject_data[\"chest\"]\n",
    "    df_chest = df_chest[df_chest[\"label\"].isin(valid_labels)]\n",
    "\n",
    "    if df_chest.empty:\n",
    "        continue\n",
    "\n",
    "    max_time = float(df_chest[\"time\"].max())\n",
    "    t = 0.0\n",
    "\n",
    "    while t + window_size_sec <= max_time:\n",
    "        try:\n",
    "            feats = extract_window_features_time_based(\n",
    "                subject_data, t, t + window_size_sec\n",
    "            )\n",
    "            if feats is not None:\n",
    "                feats[\"subject\"] = subject_id\n",
    "                all_features.append(feats)\n",
    "        except:\n",
    "            pass\n",
    "        t += step_size_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(all_features)\n",
    "df_features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_features.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed661510",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    col for col in df_features.columns if col not in [\"label\", \"subject\"]\n",
    "]\n",
    "chest_features = [\n",
    "    col\n",
    "    for col in feature_columns\n",
    "    if col.startswith((\"chest_\", \"ecg\", \"emg\", \"hr_\", \"hrv_\", \"rsp_\", \"rrv_\"))\n",
    "]\n",
    "wrist_features = [\n",
    "    col for col in feature_columns if col.startswith(\"wrist_\") or col.startswith(\"bvp_\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe942f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.to_pickle(\"wesad_features_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = df_features[\"label\"].value_counts().sort_index()\n",
    "for label in valid_labels:\n",
    "    count = label_dist.get(label, 0)\n",
    "    pct = (count / len(df_features)) * 100\n",
    "    print(f\"  {label_names[label]:12s}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n",
    "\n",
    "subject_dist = df_features.groupby([\"subject\", \"label\"]).size().unstack(fill_value=0)\n",
    "print(subject_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_label_corr = (\n",
    "    df_features[feature_columns]\n",
    "    .corrwith(df_features[\"label\"])\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "for i, (feat, corr) in enumerate(feature_label_corr.head(20).items(), 1):\n",
    "    sensor = (\n",
    "        \"CHEST\"\n",
    "        if any(\n",
    "            feat.startswith(p)\n",
    "            for p in [\"chest_\", \"ecg\", \"emg\", \"hr_\", \"hrv_\", \"rsp_\", \"rrv_\"]\n",
    "        )\n",
    "        else \"WRIST\"\n",
    "    )\n",
    "    print(f\"  {i:2d}. {feat:45s}: {corr:.4f} [{sensor}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 15, 20, 25, 30, 40, 50]\n",
    "validation_results = []\n",
    "\n",
    "print(\n",
    "    f\"\\n{'K':<8} {'Train Acc':<12} {'Val Acc':<12} {'Val F1':<12} {'Overfitting':<12}\"\n",
    ")\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=\"euclidean\", weights=\"uniform\")\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_pred = knn.predict(X_train_scaled)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "\n",
    "    val_pred = knn.predict(X_val_scaled)\n",
    "    val_acc = accuracy_score(y_val, val_pred)\n",
    "    val_f1 = f1_score(y_val, val_pred, average=\"weighted\")\n",
    "\n",
    "    diff = train_acc - val_acc\n",
    "\n",
    "    validation_results.append(\n",
    "        {\n",
    "            \"k\": k,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"overfitting\": diff,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"{k:<8} {train_acc:<12.4f} {val_acc:<12.4f} {val_f1:<12.4f} {diff:<12.4f}\")\n",
    "\n",
    "val_df = pd.DataFrame(validation_results)\n",
    "best_idx = val_df[\"val_f1\"].idxmax()\n",
    "optimal_k = int(val_df.loc[best_idx, \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d6a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_train_full = np.concatenate([y_train, y_val])\n",
    "train_val_subjects_array = np.concatenate(\n",
    "    [df_train[\"subject\"].values, df_val[\"subject\"].values]\n",
    ")\n",
    "\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "cv_scores = []\n",
    "cv_f1_scores = []\n",
    "\n",
    "\n",
    "fold_num = 1\n",
    "for train_idx, test_idx in group_kfold.split(\n",
    "    X_train_full, y_train_full, groups=train_val_subjects_array\n",
    "):\n",
    "    knn_cv = KNeighborsClassifier(\n",
    "        n_neighbors=optimal_k, metric=\"euclidean\", weights=\"uniform\"\n",
    "    )\n",
    "    knn_cv.fit(X_train_full[train_idx], y_train_full[train_idx])\n",
    "\n",
    "    y_cv_pred = knn_cv.predict(X_train_full[test_idx])\n",
    "    cv_acc = accuracy_score(y_train_full[test_idx], y_cv_pred)\n",
    "    cv_f1 = f1_score(y_train_full[test_idx], y_cv_pred, average=\"weighted\")\n",
    "\n",
    "    cv_scores.append(cv_acc)\n",
    "    cv_f1_scores.append(cv_f1)\n",
    "\n",
    "    print(f\"Fold {fold_num:<4} {cv_acc:<12.4f} {cv_f1:<12.4f}\")\n",
    "    fold_num += 1\n",
    "\n",
    "cv_scores = np.array(cv_scores)\n",
    "cv_f1_scores = np.array(cv_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a13c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_final = KNeighborsClassifier(\n",
    "    n_neighbors=optimal_k, metric=\"euclidean\", weights=\"uniform\"\n",
    ")\n",
    "\n",
    "time_start = time.time()\n",
    "knn_final.fit(X_train_full, y_train_full)\n",
    "train_time = time.time() - time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb35b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "y_pred_final = knn_final.predict(X_test_scaled)\n",
    "inference_time = time.time() - time_start\n",
    "\n",
    "accuracy_final = accuracy_score(y_test, y_pred_final)\n",
    "precision_final = precision_score(y_test, y_pred_final, average=\"weighted\")\n",
    "recall_final = recall_score(y_test, y_pred_final, average=\"weighted\")\n",
    "f1_final = f1_score(y_test, y_pred_final, average=\"weighted\")\n",
    "\n",
    "\n",
    "print(f\"{optimal_k}\")\n",
    "print(f\"{accuracy_final:.4f} ({accuracy_final*100:.2f}%)\")\n",
    "print(f\"{precision_final:.4f}\")\n",
    "print(f\"{recall_final:.4f}\")\n",
    "print(f\"{f1_final:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
